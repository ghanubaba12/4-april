{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eefc13-a93a-44c8-915f-0a16b0e83a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Describe the decision tree classifier algorithm and how it works to make predictions.\n",
    "The decision tree classifier is a popular machine learning algorithm used for classification tasks. It works by recursively partitioning the feature space into subsets that contain similar examples of the target variable, ultimately resulting in a tree-like structure that can be used to make predictions.\n",
    "\n",
    "Here's an overview of how the decision tree classifier algorithm works:\n",
    "\n",
    "Data Preparation: The first step is to gather and preprocess the data. This typically involves collecting a labeled dataset with features (also known as predictors or attributes) and their corresponding target variable (the variable to be predicted).\n",
    "\n",
    "Feature Selection: The algorithm selects the best feature from the dataset to split the data into subsets based on a criterion such as Gini impurity or entropy. The feature with the highest predictive power is chosen as the root node of the decision tree.\n",
    "\n",
    "Splitting: The selected feature is used to split the data into subsets or branches, creating child nodes. The data is partitioned into subsets based on the values of the chosen feature, such that each subset contains similar examples of the target variable.\n",
    "\n",
    "Recursive Splitting: The splitting process is repeated recursively on each child node until a stopping criterion is met. This criterion may include reaching a maximum depth of the tree, achieving a minimum number of samples in a leaf node, or achieving a minimum improvement in the criterion used for splitting.\n",
    "\n",
    "Leaf Node Assignment: Once the stopping criterion is met, the remaining nodes are designated as leaf nodes, where the final predictions are made. The majority class in each leaf node is assigned as the predicted class for that subset of data.\n",
    "\n",
    "Prediction: To make a prediction for a new example, it is passed down the decision tree from the root node to a leaf node, following the path determined by the feature values of the example. The majority class in the corresponding leaf node is assigned as the predicted class for that example.\n",
    "\n",
    "Pruning (Optional): After the decision tree is constructed, it can be pruned to prevent overfitting. Pruning involves removing unnecessary nodes from the tree that do not contribute significantly to its predictive accuracy.\n",
    "\n",
    "Prediction: Once the decision tree is constructed and pruned (if applicable), it can be used to make predictions on new, unseen examples by following the path from the root node to a leaf node based on the feature values of the example, and assigning the majority class in the corresponding leaf node as the predicted class.\n",
    "\n",
    "Overall, the decision tree classifier algorithm is a recursive process that builds a tree-like structure to represent decision rules based on the features of the data, and uses this structure to make predictions for new examples.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097a43e-ba74-499a-a7e6-1d953049c6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Provide a step-by-step explanation of the mathematical intuition behind decision tree classification.\n",
    "ans-\n",
    "Sure! Decision tree classification is a popular machine learning algorithm used for solving classification problems. The mathematical intuition behind decision tree classification can be explained in the following steps:\n",
    "\n",
    "Step 1: Data Preparation\n",
    "The first step in building a decision tree is to prepare the data. This involves collecting a labeled dataset, where each data point is associated with a class label. The dataset is divided into two parts: the training set, which is used to train the decision tree, and the test set, which is used to evaluate its performance.\n",
    "\n",
    "Step 2: Selecting the Root Node\n",
    "The decision tree starts with a root node, which represents the feature that best splits the data into different classes. The goal is to choose a feature that maximizes the information gain or minimizes the entropy, which measures the impurity of the data. Higher information gain or lower entropy indicates a better split, as it leads to more homogeneous subsets of data.\n",
    "\n",
    "Step 3: Splitting the Data\n",
    "Once the root node is selected, the data is split into subsets based on the values of the selected feature. Each subset corresponds to a branch from the root node to a child node. This process is repeated recursively for each child node until a stopping criterion is met, such as reaching a maximum depth or having pure subsets where all data points belong to the same class.\n",
    "\n",
    "Step 4: Assigning Class Labels\n",
    "At the leaf nodes of the decision tree, the majority class label of the data points in that leaf node is assigned as the predicted class label for that subset. This is done based on the class labels of the data points in that leaf node.\n",
    "\n",
    "Step 5: Handling Missing Values and Pruning\n",
    "Decision trees can handle missing values by using various techniques, such as surrogate split or imputation. Additionally, decision trees are prone to overfitting, which can be mitigated through pruning techniques, such as pre-pruning (limiting the maximum depth of the tree) or post-pruning (pruning the tree after it's fully grown and then pruning back some branches).\n",
    "\n",
    "Step 6: Predicting New Data Points\n",
    "Once the decision tree is trained and pruned, it can be used to predict the class labels of new, unseen data points. The data point is passed through the decision tree by following the split decisions based on the feature values of the data point until it reaches a leaf node, and then the majority class label of that leaf node is assigned as the predicted class label for the new data point.\n",
    "\n",
    "Step 7: Evaluating Model Performance\n",
    "The performance of the decision tree model is evaluated using the test set, which was set aside during data preparation. Common evaluation metrics for classification problems include accuracy, precision, recall, F1 score, and confusion matrix, among others.\n",
    "\n",
    "That's the step-by-step mathematical intuition behind decision tree classification. The algorithm makes decisions at each node based on the feature values of the data points, leading to a tree-like structure that represents the decision-making process for classifying data points into different classes.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a7256-2db9-450a-bd9c-e0494f8e4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. Explain how a decision tree classifier can be used to solve a binary classification problem.\n",
    "A decision tree classifier can be used to solve a binary classification problem, where the goal is to categorize examples into one of two possible classes or categories. Here's how a decision tree classifier can be used for binary classification:\n",
    "\n",
    "Data Preparation: Gather and preprocess a labeled dataset that includes features (predictors or attributes) and their corresponding binary target variable (e.g., class labels 0 and 1).\n",
    "\n",
    "Feature Selection: Use a criterion such as Gini impurity or entropy to select the best feature from the dataset to split the data into subsets based on their values.\n",
    "\n",
    "Splitting: Split the data into subsets or branches based on the chosen feature value. For example, if the chosen feature is \"age\" and the dataset contains examples of people, the data may be split into subsets of \"age < 30\" and \"age >= 30\" based on the value of 30.\n",
    "\n",
    "Recursive Splitting: Repeat the splitting process recursively on each child node until a stopping criterion is met. The criterion may include reaching a maximum depth of the tree, achieving a minimum number of samples in a leaf node, or achieving a minimum improvement in the criterion used for splitting.\n",
    "\n",
    "Leaf Node Assignment: Once the stopping criterion is met, designate the remaining nodes as leaf nodes, where the final predictions are made. Assign the majority class (0 or 1) in each leaf node as the predicted class for that subset of data.\n",
    "\n",
    "Prediction: To make a prediction for a new example, pass it down the decision tree from the root node to a leaf node, following the path determined by the feature values of the example. The majority class in the corresponding leaf node is assigned as the predicted class for that example.\n",
    "\n",
    "Pruning (Optional): After the decision tree is constructed, it can be pruned to prevent overfitting, by removing unnecessary nodes from the tree that do not contribute significantly to its predictive accuracy.\n",
    "\n",
    "Prediction: Once the decision tree is constructed and pruned (if applicable), it can be used to make predictions on new, unseen examples by following the path from the root node to a leaf node based on the feature values of the example, and assigning the majority class in the corresponding leaf node as the predicted class.\n",
    "\n",
    "The decision tree classifier can be a simple yet effective tool for binary classification problems, as it recursively partitions the feature space based on the values of the features, and uses this structure to make predictions for new examples.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687a76b1-f286-485c-b97f-6ebb0cb39187",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Discuss the geometric intuition behind decision tree classification and how it can be used to make\n",
    "predictions.\n",
    "ans-\n",
    "The geometric intuition behind decision tree classification is that it represents a hierarchical partitioning of the feature space into regions, where each region is associated with a predicted class label. The decision tree can be thought of as recursively splitting the feature space along the axes of the input features, creating a tree-like structure that captures decision rules for classifying data points.\n",
    "\n",
    "To illustrate the geometric intuition of decision tree classification, let's consider a simple binary classification problem with two input features (i.e., a 2-dimensional feature space). The decision tree will partition the feature space into rectangular regions, where each region corresponds to a decision rule that predicts the class label for data points falling within that region.\n",
    "\n",
    "At the root node of the decision tree, the feature space is split along one of the input features based on a threshold value. This creates two subsets of data points, one on each side of the split, which are then passed down to the child nodes. The process is repeated recursively at each child node until a stopping criterion is met, such as reaching a maximum depth or having pure subsets where all data points belong to the same class.\n",
    "\n",
    "The splitting of the feature space at each node can be visualized as a partitioning of the feature space into smaller rectangular regions. Each region is associated with a predicted class label based on the majority class of the data points in that region. The boundaries of these regions are aligned with the axes of the input features, resulting in a piecewise constant decision boundary in the feature space.\n",
    "\n",
    "To make predictions using the decision tree, a new data point is passed down the tree from the root node to a leaf node, following the decision rules based on the feature values of the data point. Once the data point reaches a leaf node, the predicted class label associated with that leaf node is assigned as the predicted class label for the data point.\n",
    "\n",
    "The geometric intuition of decision tree classification allows for easy interpretability, as the decision rules and the resulting decision boundaries are straightforward to understand. It also allows for capturing non-linear relationships between features and class labels, as the decision tree can make decisions based on multiple splits along different axes of the feature space. However, decision trees are prone to overfitting, as they can create overly complex decision boundaries that may not generalize well to unseen data. This is why pruning techniques and regularization methods, such as maximum depth limitation and minimum sample split, are often used to control the complexity of decision trees and improve their predictive performance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c0a8bf-d14c-4ac3-afd4-30bcb048de3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4029210095.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Q5. Define the confusion matrix and describe how it can be used to evaluate the performance of a\n",
    "classification model.\n",
    "ans-\n",
    "The confusion matrix, also known as an error matrix or a contingency table, is a table that is commonly used to evaluate the performance of a classification model. It provides a comprehensive view of the model's predictions by showing the counts of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) outcomes. Here's a breakdown of each term in the confusion matrix:\n",
    "\n",
    "True Positive (TP): The number of examples that are actually positive (belong to the positive class) and are correctly predicted as positive by the model.\n",
    "\n",
    "True Negative (TN): The number of examples that are actually negative (belong to the negative class) and are correctly predicted as negative by the model.\n",
    "\n",
    "False Positive (FP): The number of examples that are actually negative but are incorrectly predicted as positive by the model. Also known as a Type I error.\n",
    "\n",
    "False Negative (FN): The number of examples that are actually positive but are incorrectly predicted as negative by the model. Also known as a Type II error.\n",
    "\n",
    "The confusion matrix is typically presented in a tabular format with rows representing the actual class labels and columns representing the predicted class labels. It can be used to evaluate the performance of a classification model in several ways:\n",
    "\n",
    "Accuracy: The overall accuracy of the model can be calculated as (TP + TN) / (TP + TN + FP + FN), which represents the proportion of correctly predicted examples out of the total examples. Higher accuracy values indicate better performance.\n",
    "\n",
    "Precision: Precision, also known as positive predictive value, is calculated as TP / (TP + FP), which represents the proportion of true positive predictions out of the total positive predictions. Precision measures the ability of the model to correctly identify positive examples.\n",
    "\n",
    "Recall: Recall, also known as sensitivity or true positive rate, is calculated as TP / (TP + FN), which represents the proportion of true positive predictions out of the total actual positive examples. Recall measures the ability of the model to capture all the positive examples.\n",
    "\n",
    "F1-score: The F1-score is the harmonic mean of precision and recall, and is calculated as 2 * (Precision * Recall) / (Precision + Recall). It provides a balance between precision and recall, where higher values indicate better performance.\n",
    "\n",
    "Specificity: Specificity, also known as true negative rate, is calculated as TN / (TN + FP), which represents the proportion of true negative predictions out of the total actual negative examples. Specificity measures the ability of the model to correctly identify negative examples.\n",
    "\n",
    "False Positive Rate (FPR): FPR is calculated as FP / (FP + TN), which represents the proportion of false positive predictions out of the total actual negative examples. FPR measures the rate of false positives made by the model.\n",
    "\n",
    "False Negative Rate (FNR): FNR is calculated as FN / (FN + TP), which represents the proportion of false negative predictions out of the total actual positive examples. FNR measures the rate of false negatives made by the model.\n",
    "\n",
    "By examining the values in the confusion matrix and calculating these performance metrics, one can gain insights into the strengths and weaknesses of the classification model. It helps in understanding how well the model is performing in terms of correctly predicting the positive and negative examples, and identifying any potential biases or errors. Based on the results from the confusion matrix, appropriate actions can be taken to improve the model's performance, such as adjusting the model's parameters, using different algorithms, or collecting more data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68156604-fe8e-41b7-8851-538285407897",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 2) (460861747.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [3], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    calculated from it.Sure! A confusion matrix is a table used to evaluate the performance of a classification model by showing the counts of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions for a set of data points. Here's an example of a confusion matrix:\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 2)\n"
     ]
    }
   ],
   "source": [
    "Q6. Provide an example of a confusion matrix and explain how precision, recall, and F1 score can be\n",
    "calculated from it.Sure! A confusion matrix is a table used to evaluate the performance of a classification model by showing the counts of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) predictions for a set of data points. Here's an example of a confusion matrix:\n",
    "\n",
    "css\n",
    "Copy code\n",
    "Actual/Predicted   Class A    Class B\n",
    "Class A             TP         FP\n",
    "Class B             FN         TN\n",
    "Let's break down the components of the confusion matrix:\n",
    "\n",
    "True Positive (TP): The number of instances where the actual class is positive (e.g., Class A) and the predicted class is also positive (e.g., correctly predicted as Class A).\n",
    "True Negative (TN): The number of instances where the actual class is negative (e.g., Class B) and the predicted class is also negative (e.g., correctly predicted as Class B).\n",
    "False Positive (FP): The number of instances where the actual class is negative (e.g., Class B) but the predicted class is positive (e.g., incorrectly predicted as Class A).\n",
    "False Negative (FN): The number of instances where the actual class is positive (e.g., Class A) but the predicted class is negative (e.g., incorrectly predicted as Class B).\n",
    "Precision, recall, and F1 score are commonly used evaluation metrics that can be calculated from the values in the confusion matrix as follows:\n",
    "\n",
    "Precision (also known as positive predictive value) is the ratio of true positives to the sum of true positives and false positives. It measures the accuracy of positive predictions made by the model.\n",
    "makefile\n",
    "Copy code\n",
    "Precision = TP / (TP + FP)\n",
    "Recall (also known as sensitivity or true positive rate) is the ratio of true positives to the sum of true positives and false negatives. It measures the ability of the model to correctly identify positive instances.\n",
    "makefile\n",
    "Copy code\n",
    "Recall = TP / (TP + FN)\n",
    "F1 score is the harmonic mean of precision and recall, and it provides a balanced measure of both precision and recall. It is often used when both precision and recall are important.\n",
    "mathematica\n",
    "Copy code\n",
    "F1 score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "The values of precision, recall, and F1 score range from 0 to 1, with higher values indicating better performance. A higher precision indicates fewer false positives, while a higher recall indicates fewer false negatives. The F1 score combines both precision and recall, providing a single metric to evaluate the overall performance of a classification model based on both false positives and false negatives.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262f619-a5a0-4207-af29-6bdd2f043358",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Discuss the importance of choosing an appropriate evaluation metric for a classification problem and\n",
    "explain how this can be done.\n",
    "ans-Choosing an appropriate evaluation metric for a classification problem is crucial as it directly impacts how the performance of a model is assessed. Different evaluation metrics focus on different aspects of model performance, and the choice of metric depends on the specific goals and requirements of the classification problem. It is important to carefully select an evaluation metric that aligns with the problem at hand, as using an inappropriate metric may lead to misleading or inaccurate conclusions about the model's performance.\n",
    "\n",
    "Here are some key considerations for choosing an appropriate evaluation metric for a classification problem:\n",
    "\n",
    "Nature of the problem: Consider the nature of the classification problem and the specific goals of the analysis. For example, in a medical diagnosis problem where false positives and false negatives have different consequences, the evaluation metric should reflect the importance of both precision and recall. In a spam detection problem, false positives may be less critical, and higher precision might be more important. Understanding the nuances of the problem and the consequences of different types of errors can help in choosing an appropriate evaluation metric.\n",
    "\n",
    "Class imbalance: Class imbalance refers to the situation where one class is significantly more prevalent than the other in the dataset. In such cases, accuracy may not be a reliable metric as it can be skewed by the majority class. Metrics such as precision, recall, F1-score, and area under the Receiver Operating Characteristic (ROC) curve may be more appropriate as they take into account both the true positive rate and false positive rate, and are less affected by class imbalance.\n",
    "\n",
    "Trade-offs between metrics: Different evaluation metrics may have trade-offs, and it is important to understand the balance between them. For example, precision and recall are often inversely related, meaning that improving one may degrade the other. The F1-score is a harmonic mean of precision and recall, providing a balanced measure, but may not always be suitable depending on the problem. Understanding these trade-offs can help in selecting the most appropriate evaluation metric for the problem at hand.\n",
    "\n",
    "Interpretability and communication: The choice of evaluation metric should also take into consideration the interpretability and communication of the results. Some metrics, such as accuracy, are easy to understand and communicate to stakeholders who may not have a technical background. However, other metrics like the Matthews correlation coefficient (MCC) or the Cohen's kappa may require additional explanation or understanding of statistical concepts. It is important to choose a metric that aligns with the communication needs of the stakeholders and effectively conveys the model's performance.\n",
    "\n",
    "Once the appropriate evaluation metric(s) are identified, they can be used to assess the performance of the classification model during model development and testing phases. It is important to consistently report and interpret the results based on the chosen evaluation metric(s) to ensure that the model's performance is objectively evaluated and compared against the desired performance goals. Additionally, it is recommended to use multiple evaluation metrics to gain a more comprehensive understanding of the model's performance from different perspectives, and to make informed decisions about model selection, parameter tuning, and overall model improvement.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af7ee0f-783f-4466-8c9b-d8ae0f9f12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. Provide an example of a classification problem where precision is the most important metric, and\n",
    "explain why.\n",
    "ans-Sure! One example of a classification problem where precision is the most important metric is a medical diagnosis scenario, specifically for a disease that has serious consequences and requires immediate treatment. Let's consider the example of a cancer screening test where the goal is to classify individuals as either \"positive\" (indicating the presence of cancer) or \"negative\" (indicating the absence of cancer).\n",
    "\n",
    "In such a case, precision is crucial because false positives (i.e., classifying a healthy person as positive for cancer) can have severe consequences, such as unnecessary medical interventions (e.g., surgeries, radiation, chemotherapy) and psychological distress for the patients. It can also lead to unnecessary healthcare costs and resource utilization.\n",
    "\n",
    "By prioritizing precision as the most important metric, we aim to minimize the false positive rate (FP) in the confusion matrix, which corresponds to the ratio of false positives to the sum of true negatives and false positives. A high precision value indicates a low false positive rate, meaning that the model is accurately identifying true positive cases (i.e., individuals with cancer) while minimizing false positives (i.e., healthy individuals incorrectly classified as having cancer).\n",
    "\n",
    "In this case, it is acceptable to have a lower recall (i.e., sensitivity or true positive rate), as some true positive cases may be missed (i.e., false negatives) due to the emphasis on precision. While false negatives may result in missed diagnoses, the priority is to avoid false positives, which can lead to unnecessary and potentially harmful treatments.\n",
    "\n",
    "In summary, in a medical diagnosis scenario where the consequences of false positives are severe, precision becomes the most important metric as it ensures accurate identification of positive cases while minimizing false positives and their associated consequences.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e78fc-197d-4ac0-8779-e88083169afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. Provide an example of a classification problem where recall is the most important metric, and explain\n",
    "why.\n",
    "ans-\n",
    "One example of a classification problem where recall is the most important metric is in cancer detection. Specifically, consider a scenario where a machine learning model is trained to classify whether a patient has cancer (positive class) or not (negative class) based on certain features such as age, family history, and results of medical tests.\n",
    "\n",
    "In this case, recall, also known as sensitivity or true positive rate, is the most important metric because the consequence of missing a true positive, i.e., a patient with cancer being classified as cancer-free (false negative), can have serious implications. A false negative in this context means that the model fails to identify a patient with cancer, leading to a delay in diagnosis and potentially delaying or denying appropriate treatment, which could have a detrimental impact on the patient's health outcomes.\n",
    "\n",
    "On the other hand, a false positive, where a patient without cancer is classified as having cancer, may lead to further tests or unnecessary treatments, but it is generally less harmful compared to a false negative in the context of cancer detection.\n",
    "\n",
    "By prioritizing recall as the primary evaluation metric in this scenario, the model aims to minimize false negatives, maximizing the ability to detect all true positive cases of cancer, and minimizing the risk of missing patients who actually have cancer. This prioritization ensures that the model's performance is optimized for sensitivity, which is crucial in situations where identifying all cases of the positive class, even at the cost of some false positives, is more important to avoid missing any true positive cases.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff176f-512b-4c22-a78c-936e4452adba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24140961-13a8-4b94-a8a4-c3e6c3980ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
